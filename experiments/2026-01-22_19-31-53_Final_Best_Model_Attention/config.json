{
    "experiment_name": "Final_Best_Model_Attention",
    "seed": 42,
    "learning_rate": 0.0003,
    "batch_size": 32,
    "embed_size": 512,
    "hidden_size": 512,
    "num_layers": 1,
    "num_epochs": 20,
    "save_model": true,
    "num_workers": 6,
    "image_size": 224,
    "resize_dim": 232,
    "train_cnn": false,
    "bleu_every_n_epochs": 1,
    "load_model": false,
    "checkpoint_path": "experiments/2026-01-17_19-04-17_ResNet101_LSTM_v1/weights/checkpoint_epoch_3.pth.tar",
    "optimizer": "Adam",
    "rnn_type": "LSTM",
    "patience": 5,
    "system": {
        "device_fallback_enabled": true
    },
    "dropout": 0.5,
    "model_type": "attention",
    "attention_dim": 512
}