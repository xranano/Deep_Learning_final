{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1328792,"sourceType":"datasetVersion","datasetId":771078}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"print(\"Setting up Kaggle environment for Image Captioning...\")\n\n# Install dependencies\n!pip install torch torchvision pillow numpy pandas tqdm\n\n# Import libraries\nimport os\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport numpy as np\nimport random\nfrom collections import Counter\nimport json\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"\\n‚úì Environment ready!\")\nprint(f\"Working directory: {os.getcwd()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T14:50:07.128851Z","iopub.execute_input":"2026-01-19T14:50:07.129740Z","iopub.status.idle":"2026-01-19T14:50:17.070134Z","shell.execute_reply.started":"2026-01-19T14:50:07.129704Z","shell.execute_reply":"2026-01-19T14:50:17.069422Z"}},"outputs":[{"name":"stdout","text":"Setting up Kaggle environment for Image Captioning...\nRequirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\nRequirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\nRequirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.3)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.10.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\nRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\nRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\nRequirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\nRequirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\nRequirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\nRequirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n\n‚úì Environment ready!\nWorking directory: /kaggle/working/Deep_Learning_final\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# ============================================================================\n# CELL 2: KAGGLE CONFIGURATION\n# ============================================================================\n\nprint(\"Setting Kaggle paths...\")\n\n# KAGGLE PATHS (Update these if your dataset name is different)\nKAGGLE_INPUT_DIR = \"/kaggle/input\"\nKAGGLE_WORKING_DIR = \"/kaggle/working\"\n\n# Auto-detect Flickr8k dataset\ndatasets = os.listdir(KAGGLE_INPUT_DIR)\nflickr_dataset = None\nfor d in datasets:\n    if \"flickr\" in d.lower() or \"flickr8k\" in d.lower():\n        flickr_dataset = d\n        break\n\nif flickr_dataset:\n    DATASET_PATH = f\"{KAGGLE_INPUT_DIR}/{flickr_dataset}\"\n    # Try to find the actual structure\n    possible_paths = [\n        f\"{DATASET_PATH}/flickr8k\",\n        DATASET_PATH,\n        f\"{DATASET_PATH}/Flickr8k\"\n    ]\n    \n    for path in possible_paths:\n        if os.path.exists(f\"{path}/captions.txt\"):\n            DATASET_PATH = path\n            break\n    \n    IMAGE_DIR = f\"{DATASET_PATH}/Images\"\n    ANNOTATION_FILE = f\"{DATASET_PATH}/captions.txt\"\n    \n    # If Images folder doesn't exist, check for images folder\n    if not os.path.exists(IMAGE_DIR):\n        if os.path.exists(f\"{DATASET_PATH}/images\"):\n            IMAGE_DIR = f\"{DATASET_PATH}/images\"\n        elif os.path.exists(f\"{DATASET_PATH}/Flickr8k_Dataset\"):\n            IMAGE_DIR = f\"{DATASET_PATH}/Flickr8k_Dataset\"\n    \n    print(f\"‚úì Found dataset: {flickr_dataset}\")\n    print(f\"  Images: {IMAGE_DIR}\")\n    print(f\"  Captions: {ANNOTATION_FILE}\")\nelse:\n    print(\"‚ö†Ô∏è No Flickr dataset found. Please add 'flickr8k' dataset via '+ Add data'\")\n    IMAGE_DIR = \"/kaggle/input/flickr8k/Images\"  # Default fallback\n    ANNOTATION_FILE = \"/kaggle/input/flickr8k/captions.txt\"\n\n# Output directories\nEXPERIMENT_DIR = f\"{KAGGLE_WORKING_DIR}/experiments\"\nMODEL_SAVE_DIR = f\"{KAGGLE_WORKING_DIR}/models\"\nos.makedirs(EXPERIMENT_DIR, exist_ok=True)\nos.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n\nprint(f\"\\n‚úì Output directories created in {KAGGLE_WORKING_DIR}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T14:50:38.447018Z","iopub.execute_input":"2026-01-19T14:50:38.447603Z","iopub.status.idle":"2026-01-19T14:50:38.471917Z","shell.execute_reply.started":"2026-01-19T14:50:38.447575Z","shell.execute_reply":"2026-01-19T14:50:38.471285Z"}},"outputs":[{"name":"stdout","text":"Setting Kaggle paths...\n‚úì Found dataset: flickr8kimagescaptions\n  Images: /kaggle/input/flickr8kimagescaptions/flickr8k/images\n  Captions: /kaggle/input/flickr8kimagescaptions/flickr8k/captions.txt\n\n‚úì Output directories created in /kaggle/working\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# ============================================================================\n# CELL 3: DATA LOADER (Modified for Kaggle)\n# ============================================================================\n\nclass Vocabulary:\n    def __init__(self, freq_threshold):\n        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n        self.freq_threshold = freq_threshold\n\n    def __len__(self):\n        return len(self.itos)\n\n    @staticmethod\n    def tokenizer_eng(text):\n        return [tok.lower() for tok in text.replace(\".\", \" .\").replace(\",\", \" ,\").split()]\n\n    def build_vocabulary(self, sentence_list):\n        frequencies = {}\n        idx = 4\n\n        for sentence in sentence_list:\n            for word in self.tokenizer_eng(sentence):\n                if word not in frequencies:\n                    frequencies[word] = 1\n                else:\n                    frequencies[word] += 1\n\n                if frequencies[word] == self.freq_threshold:\n                    self.stoi[word] = idx\n                    self.itos[idx] = word\n                    idx += 1\n\n    def numericalize(self, text):\n        tokenized_text = self.tokenizer_eng(text)\n        return [\n            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n            for token in tokenized_text\n        ]\n\n\nclass FlickrDataset(Dataset):\n    def __init__(self, root_dir, imgs, captions, vocab, transform=None):\n        self.root_dir = root_dir\n        self.imgs = imgs\n        self.captions = captions\n        self.vocab = vocab\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.imgs)\n\n    def __getitem__(self, index):\n        caption = self.captions[index]\n        img_id = self.imgs[index]\n        img_path = os.path.join(self.root_dir, img_id)\n        \n        image = Image.open(img_path).convert(\"RGB\")\n\n        if self.transform is not None:\n            image = self.transform(image)\n\n        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n        numericalized_caption += self.vocab.numericalize(caption)\n        numericalized_caption.append(self.vocab.stoi[\"<EOS>\"])\n\n        return image, torch.tensor(numericalized_caption)\n\n\nclass MyCollate:\n    def __init__(self, pad_idx):\n        self.pad_idx = pad_idx\n\n    def __call__(self, batch):\n        imgs = [item[0].unsqueeze(0) for item in batch]\n        imgs = torch.cat(imgs, dim=0)\n        targets = [item[1] for item in batch]\n        targets = pad_sequence(targets, batch_first=False, padding_value=self.pad_idx)\n        return imgs, targets\n\n\ndef get_loaders(\n    root_folder=None,\n    annotation_file=None,\n    transform=None,\n    batch_size=32,\n    num_workers=2,\n    shuffle=True,\n    pin_memory=True,\n    test_size=0.1,\n    val_size=0.1,\n    freq_threshold=5\n):\n    \"\"\"Modified for Kaggle - uses global paths if None provided\"\"\"\n    \n    # Use Kaggle paths if not specified\n    if root_folder is None:\n        root_folder = IMAGE_DIR\n    if annotation_file is None:\n        annotation_file = ANNOTATION_FILE\n    \n    all_imgs = []\n    all_captions = []\n    \n    print(f\"Loading data from: {annotation_file}\")\n    with open(annotation_file, 'r', encoding='utf-8') as f:\n        lines = f.readlines()\n        if lines and \"image,caption\" in lines[0]:\n            lines = lines[1:]\n        \n        for line in lines:\n            parts = line.strip().split(',', 1) \n            if len(parts) == 2:\n                all_imgs.append(parts[0])\n                all_captions.append(parts[1])\n\n    print(f\"Loaded {len(all_imgs)} image-caption pairs\")\n    \n    unique_imgs = list(set(all_imgs))\n    random.seed(42)\n    random.shuffle(unique_imgs)\n    \n    total_imgs = len(unique_imgs)\n    v_count = int(total_imgs * val_size)\n    t_count = int(total_imgs * test_size)\n    train_count = total_imgs - v_count - t_count\n    \n    train_img_ids = set(unique_imgs[:train_count])\n    val_img_ids = set(unique_imgs[train_count:train_count+v_count])\n    test_img_ids = set(unique_imgs[train_count+v_count:])\n    \n    train_imgs, train_caps = [], []\n    val_imgs, val_caps = [], []\n    test_imgs, test_caps = [], []\n    \n    for img, cap in zip(all_imgs, all_captions):\n        if img in train_img_ids:\n            train_imgs.append(img)\n            train_caps.append(cap)\n        elif img in val_img_ids:\n            val_imgs.append(img)\n            val_caps.append(cap)\n        elif img in test_img_ids:\n            test_imgs.append(img)\n            test_caps.append(cap)\n            \n    print(f\"Split: Train={len(train_imgs)}, Val={len(val_imgs)}, Test={len(test_imgs)}\")\n    \n    vocab = Vocabulary(freq_threshold)\n    vocab.build_vocabulary(train_caps)\n    \n    print(f\"Vocabulary size: {len(vocab)}\")\n    \n    train_dataset = FlickrDataset(root_folder, train_imgs, train_caps, vocab, transform=transform)\n    val_dataset = FlickrDataset(root_folder, val_imgs, val_caps, vocab, transform=transform)\n    test_dataset = FlickrDataset(root_folder, test_imgs, test_caps, vocab, transform=transform)\n    \n    pad_idx = vocab.stoi[\"<PAD>\"]\n\n    # Kaggle optimization: adjust workers\n    num_workers = min(num_workers, 2)  # Kaggle works better with fewer workers\n    \n    train_loader = DataLoader(\n        train_dataset, batch_size=batch_size, num_workers=num_workers, \n        shuffle=shuffle, pin_memory=pin_memory, collate_fn=MyCollate(pad_idx)\n    )\n    val_loader = DataLoader(\n        val_dataset, batch_size=batch_size, num_workers=num_workers, \n        shuffle=False, pin_memory=pin_memory, collate_fn=MyCollate(pad_idx)\n    )\n    test_loader = DataLoader(\n        test_dataset, batch_size=batch_size, num_workers=num_workers, \n        shuffle=False, pin_memory=pin_memory, collate_fn=MyCollate(pad_idx)\n    )\n\n    return train_loader, val_loader, test_loader, vocab\n\nprint(\"‚úì Data loader module ready\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T14:51:03.325923Z","iopub.execute_input":"2026-01-19T14:51:03.326689Z","iopub.status.idle":"2026-01-19T14:51:03.345821Z","shell.execute_reply.started":"2026-01-19T14:51:03.326660Z","shell.execute_reply":"2026-01-19T14:51:03.345095Z"}},"outputs":[{"name":"stdout","text":"‚úì Data loader module ready\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# ============================================================================\n# CELL 4: MODEL ARCHITECTURE\n# ============================================================================\n\nclass CNNtoRNN(nn.Module):\n    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, dropout=0.5):\n        super(CNNtoRNN, self).__init__()\n        \n        # Encoder CNN (ResNet101 without final layer)\n        resnet = models.resnet101(weights=models.ResNet101_Weights.DEFAULT)\n        modules = list(resnet.children())[:-2]  # Remove avgpool and fc\n        self.cnn = nn.Sequential(*modules)\n        \n        # Adaptive pooling\n        self.adaptive_pool = nn.AdaptiveAvgPool2d((14, 14))\n        \n        # Reduce channels to match embedding size\n        self.channel_reducer = nn.Conv2d(2048, embed_size, kernel_size=1)\n        \n        # Decoder LSTM\n        self.rnn = nn.LSTM(\n            input_size=embed_size,\n            hidden_size=hidden_size,\n            num_layers=num_layers,\n            dropout=dropout if num_layers > 1 else 0,\n            batch_first=False\n        )\n        \n        # Attention mechanism\n        self.attention = nn.Linear(hidden_size + embed_size, 14*14)\n        self.attention_combine = nn.Linear(hidden_size + embed_size, hidden_size)\n        \n        # Output layers\n        self.fc_out = nn.Linear(hidden_size, vocab_size)\n        self.dropout = nn.Dropout(dropout)\n        \n        # Embedding layer\n        self.embed = nn.Embedding(vocab_size, embed_size)\n        \n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.embed_size = embed_size\n\n    def forward(self, images, captions):\n        # CNN feature extraction\n        features = self.cnn(images)\n        features = self.adaptive_pool(features)\n        features = self.channel_reducer(features)\n        \n        # Reshape features: (batch_size, embed_size, 14, 14) -> (196, batch_size, embed_size)\n        batch_size = features.size(0)\n        features = features.view(batch_size, self.embed_size, -1).permute(2, 0, 1)\n        \n        # Initialize hidden states\n        h = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(images.device)\n        c = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(images.device)\n        \n        # Embed captions\n        embeddings = self.embed(captions)\n        \n        # Sequence length\n        seq_length = captions.size(0)\n        batch_size = captions.size(1)\n        \n        # Prepare outputs\n        outputs = torch.zeros(seq_length, batch_size, len(self.embed.weight)).to(images.device)\n        \n        # Process sequence\n        for t in range(seq_length):\n            # Attention mechanism\n            combined = torch.cat((h[-1], embeddings[t]), dim=1)\n            attention_weights = self.attention(combined)\n            attention_weights = attention_weights.view(batch_size, 1, 14*14)\n            attention_weights = torch.softmax(attention_weights, dim=2)\n            \n            # Apply attention to features\n            context = torch.bmm(attention_weights, features.permute(1, 0, 2))\n            context = context.squeeze(1)\n            \n            # Combine with embedding\n            combined = torch.cat((h[-1], context), dim=1)\n            attention_combined = self.attention_combine(combined)\n            \n            # LSTM step\n            lstm_input = attention_combined.unsqueeze(0)\n            out, (h, c) = self.rnn(lstm_input, (h, c))\n            \n            # Output\n            out = self.fc_out(self.dropout(out.squeeze(0)))\n            outputs[t] = out\n        \n        return outputs\n\n    def caption_image(self, image, vocab, max_length=50, device=\"cuda\"):\n        \"\"\"Generate caption for a single image\"\"\"\n        self.eval()\n        \n        with torch.no_grad():\n            # Extract features\n            features = self.cnn(image)\n            features = self.adaptive_pool(features)\n            features = self.channel_reducer(features)\n            features = features.view(1, self.embed_size, -1).permute(2, 0, 1)\n            \n            # Initialize\n            h = torch.zeros(self.num_layers, 1, self.hidden_size).to(device)\n            c = torch.zeros(self.num_layers, 1, self.hidden_size).to(device)\n            \n            # Start token\n            word = torch.tensor([[vocab.stoi[\"<SOS>\"]]]).to(device)\n            caption = []\n            \n            for _ in range(max_length):\n                embeddings = self.embed(word)\n                \n                # Attention\n                combined = torch.cat((h[-1], embeddings.squeeze(1)), dim=1)\n                attention_weights = self.attention(combined)\n                attention_weights = torch.softmax(attention_weights, dim=1)\n                context = torch.bmm(attention_weights.unsqueeze(1), features.permute(1, 0, 2))\n                context = context.squeeze(1)\n                \n                # Combine\n                combined = torch.cat((h[-1], context), dim=1)\n                attention_combined = self.attention_combine(combined)\n                \n                # LSTM\n                lstm_input = attention_combined.unsqueeze(0)\n                out, (h, c) = self.rnn(lstm_input, (h, c))\n                \n                # Predict next word\n                output = self.fc_out(out.squeeze(0))\n                predicted = output.argmax(1)\n                \n                word = predicted.unsqueeze(0)\n                \n                # Stop if end token\n                if vocab.itos[predicted.item()] == \"<EOS>\":\n                    break\n                    \n                caption.append(vocab.itos[predicted.item()])\n        \n        return \" \".join(caption)\n\nprint(\"‚úì Model architecture ready\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T14:51:52.939817Z","iopub.execute_input":"2026-01-19T14:51:52.940129Z","iopub.status.idle":"2026-01-19T14:51:52.956126Z","shell.execute_reply.started":"2026-01-19T14:51:52.940106Z","shell.execute_reply":"2026-01-19T14:51:52.955451Z"}},"outputs":[{"name":"stdout","text":"‚úì Model architecture ready\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# ============================================================================\n# CELL 5: TRAINING SCRIPT (Kaggle Optimized) - UPDATED\n# ============================================================================\n\nimport time\nfrom datetime import datetime\n\nclass ExperimentLogger:\n    def __init__(self, exp_name):\n        self.exp_name = exp_name\n        self.exp_dir = os.path.join(EXPERIMENT_DIR, exp_name)\n        os.makedirs(self.exp_dir, exist_ok=True)\n        \n        self.log_file = os.path.join(self.exp_dir, \"training_log.txt\")\n        self.config_file = os.path.join(self.exp_dir, \"config.json\")\n        \n        print(f\"[Experiment] Initialized: {self.exp_dir}\")\n    \n    def log_config(self, config):\n        with open(self.config_file, 'w') as f:\n            json.dump(config, f, indent=2)\n        print(\"[Experiment] Config saved.\")\n    \n    def log_epoch(self, epoch, train_loss, val_loss, lr):\n        log_line = f\"Epoch {epoch:03d} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | LR: {lr:.6f}\"\n        print(log_line)\n        \n        with open(self.log_file, 'a') as f:\n            f.write(log_line + \"\\n\")\n\n\ndef train_model(\n    num_epochs=10,\n    batch_size=32,\n    learning_rate=3e-4,\n    embed_size=512,\n    hidden_size=512,\n    num_layers=1,\n    grad_clip=5.0,\n    save_every=1\n):\n    \"\"\"Main training function optimized for Kaggle - FIXED VERSION\"\"\"\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"STARTING TRAINING ON KAGGLE\")\n    print(\"=\"*60)\n    \n    # Check GPU\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n    \n    if device.type == \"cuda\":\n        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n        print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n    \n    # Create experiment\n    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n    exp_name = f\"{timestamp}_ResNet101_LSTM_v1\"\n    logger = ExperimentLogger(exp_name)\n    \n    # Config\n    config = {\n        \"num_epochs\": num_epochs,\n        \"batch_size\": batch_size,\n        \"learning_rate\": learning_rate,\n        \"embed_size\": embed_size,\n        \"hidden_size\": hidden_size,\n        \"num_layers\": num_layers,\n        \"grad_clip\": grad_clip,\n        \"dataset\": os.path.basename(IMAGE_DIR),\n        \"device\": str(device),\n        \"timestamp\": timestamp\n    }\n    logger.log_config(config)\n    \n    # Data transforms\n    transform = transforms.Compose([\n        transforms.Resize((356, 356)),\n        transforms.RandomCrop((299, 299)),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n    ])\n    \n    # Get data loaders\n    print(\"\\nInitializing Loaders...\")\n    train_loader, val_loader, test_loader, vocab = get_loaders(\n        transform=transform,\n        batch_size=batch_size,\n        num_workers=2,\n        shuffle=True,\n        pin_memory=True\n    )\n    \n    # Initialize model\n    model = CNNtoRNN(\n        embed_size=embed_size,\n        hidden_size=hidden_size,\n        vocab_size=len(vocab),\n        num_layers=num_layers\n    ).to(device)\n    \n    # Loss and optimizer\n    criterion = nn.CrossEntropyLoss(ignore_index=vocab.stoi[\"<PAD>\"])\n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n    \n    # FIXED: Remove 'verbose' parameter from ReduceLROnPlateau\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode='min', factor=0.5, patience=2\n    )\n    \n    # Training loop\n    print(f\"\\nStarting training for {num_epochs} epochs...\")\n    best_val_loss = float('inf')\n    \n    for epoch in range(num_epochs):\n        start_time = time.time()\n        \n        # Training phase\n        model.train()\n        train_losses = []\n        \n        for batch_idx, (images, captions) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")):\n            images = images.to(device)\n            captions = captions.to(device)\n            \n            # Forward pass\n            outputs = model(images, captions[:-1])\n            loss = criterion(\n                outputs.reshape(-1, outputs.shape[2]),\n                captions[1:].reshape(-1)\n            )\n            \n            # Backward pass\n            optimizer.zero_grad()\n            loss.backward()\n            \n            # Gradient clipping\n            if grad_clip:\n                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n            \n            optimizer.step()\n            train_losses.append(loss.item())\n            \n            # Memory management for Kaggle\n            if batch_idx % 50 == 0 and device.type == \"cuda\":\n                torch.cuda.empty_cache()\n        \n        avg_train_loss = np.mean(train_losses)\n        \n        # Validation phase\n        model.eval()\n        val_losses = []\n        \n        with torch.no_grad():\n            for images, captions in val_loader:\n                images = images.to(device)\n                captions = captions.to(device)\n                \n                outputs = model(images, captions[:-1])\n                loss = criterion(\n                    outputs.reshape(-1, outputs.shape[2]),\n                    captions[1:].reshape(-1)\n                )\n                val_losses.append(loss.item())\n        \n        avg_val_loss = np.mean(val_losses)\n        scheduler.step(avg_val_loss)\n        current_lr = optimizer.param_groups[0]['lr']\n        \n        # Logging\n        epoch_time = time.time() - start_time\n        logger.log_epoch(epoch+1, avg_train_loss, avg_val_loss, current_lr)\n        print(f\"Time: {epoch_time:.1f}s\")\n        \n        # Save best model\n        if avg_val_loss < best_val_loss:\n            best_val_loss = avg_val_loss\n            model_path = os.path.join(MODEL_SAVE_DIR, f\"best_model_{exp_name}.pth\")\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'val_loss': avg_val_loss,\n                'vocab': vocab,\n                'config': config\n            }, model_path)\n            print(f\"‚úì Saved best model (val_loss: {avg_val_loss:.4f})\")\n        \n        # Periodic save\n        if (epoch + 1) % save_every == 0:\n            model_path = os.path.join(MODEL_SAVE_DIR, f\"model_epoch_{epoch+1}_{exp_name}.pth\")\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'val_loss': avg_val_loss,\n                'config': config\n            }, model_path)\n        \n        # Early stopping check\n        if current_lr < 1e-6:\n            print(\"Learning rate too low, stopping early.\")\n            break\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"TRAINING COMPLETE\")\n    print(f\"Best validation loss: {best_val_loss:.4f}\")\n    print(f\"Models saved in: {MODEL_SAVE_DIR}\")\n    print(\"=\"*60)\n    \n    return model, vocab\n\n\n# Run training with Kaggle-optimized parameters\nif __name__ == \"__main__\":\n    print(\"Testing the fixed training function...\")\n    \n    # For quick testing on Kaggle, use fewer epochs\n    try:\n        model, vocab = train_model(\n            num_epochs=3,  # Start with 3 epochs for testing\n            batch_size=16,  # Smaller batch for testing\n            learning_rate=3e-4,\n            embed_size=256,  # Smaller for faster training\n            hidden_size=256,\n            num_layers=1,\n            grad_clip=5.0,\n            save_every=1\n        )\n        print(\"\\n‚úÖ Training completed successfully!\")\n        \n        # Show model info\n        print(f\"\\nModel trained with vocabulary size: {len(vocab)}\")\n        print(f\"Check /kaggle/working/ for saved models\")\n        \n    except Exception as e:\n        print(f\"\\n‚ùå Error during training: {e}\")\n        print(\"\\nTrying simplified training from Cell 8 instead...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T14:58:34.301576Z","iopub.execute_input":"2026-01-19T14:58:34.302387Z","iopub.status.idle":"2026-01-19T15:53:18.626517Z","shell.execute_reply.started":"2026-01-19T14:58:34.302360Z","shell.execute_reply":"2026-01-19T15:53:18.625520Z"}},"outputs":[{"name":"stdout","text":"Testing the fixed training function...\n\n============================================================\nSTARTING TRAINING ON KAGGLE\n============================================================\nUsing device: cuda\nGPU: Tesla T4\nGPU Memory: 15.83 GB\n[Experiment] Initialized: /kaggle/working/experiments/2026-01-19_14-58-34_ResNet101_LSTM_v1\n[Experiment] Config saved.\n\nInitializing Loaders...\nLoading data from: /kaggle/input/flickr8kimagescaptions/flickr8k/captions.txt\nLoaded 40455 image-caption pairs\nSplit: Train=32365, Val=4045, Test=4045\nVocabulary size: 2732\n\nStarting training for 3 epochs...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2023/2023 [17:37<00:00,  1.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 001 | Train Loss: 4.2247 | Val Loss: 3.6429 | LR: 0.000300\nTime: 1099.4s\n‚úì Saved best model (val_loss: 3.6429)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2023/2023 [17:28<00:00,  1.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 002 | Train Loss: 3.6447 | Val Loss: 3.4178 | LR: 0.000300\nTime: 1090.6s\n‚úì Saved best model (val_loss: 3.4178)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2023/2023 [17:26<00:00,  1.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 003 | Train Loss: 3.4721 | Val Loss: 3.2941 | LR: 0.000300\nTime: 1088.5s\n‚úì Saved best model (val_loss: 3.2941)\n\n============================================================\nTRAINING COMPLETE\nBest validation loss: 3.2941\nModels saved in: /kaggle/working/models\n============================================================\n\n‚úÖ Training completed successfully!\n\nModel trained with vocabulary size: 2732\nCheck /kaggle/working/ for saved models\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# ============================================================================\n# CELL 6: INFERENCE & EVALUATION (Kaggle Version) - COMPLETE\n# ============================================================================\n\nfrom collections import Counter\nimport numpy as np\nfrom typing import List, Dict\nimport json\n\ndef load_trained_model(model_path, device=\"cuda\"):\n    \"\"\"Load a trained model for inference\"\"\"\n    checkpoint = torch.load(model_path, map_location=device)\n    \n    # Get config\n    config = checkpoint['config']\n    vocab = checkpoint['vocab']\n    \n    # Create model\n    model = CNNtoRNN(\n        embed_size=config['embed_size'],\n        hidden_size=config['hidden_size'],\n        vocab_size=len(vocab),\n        num_layers=config['num_layers']\n    ).to(device)\n    \n    # Load weights\n    model.load_state_dict(checkpoint['model_state_dict'])\n    model.eval()\n    \n    print(f\"‚úì Loaded model from epoch {checkpoint['epoch']}\")\n    print(f\"  Validation loss: {checkpoint['val_loss']:.4f}\")\n    print(f\"  Vocabulary size: {len(vocab)}\")\n    \n    return model, vocab\n\n\ndef generate_captions_for_test_set(model, vocab, test_loader, device=\"cuda\", num_samples=5):\n    \"\"\"Generate captions for test images\"\"\"\n    model.eval()\n    results = []\n    \n    with torch.no_grad():\n        for idx, (images, captions) in enumerate(test_loader):\n            if idx >= num_samples:\n                break\n                \n            images = images.to(device)\n            \n            # Get true captions\n            true_captions = []\n            for i in range(captions.size(1)):\n                caption_tokens = []\n                for token in captions[:, i]:\n                    word = vocab.itos[token.item()]\n                    if word == \"<EOS>\":\n                        break\n                    if word not in [\"<SOS>\", \"<PAD>\"]:\n                        caption_tokens.append(word)\n                if caption_tokens:  # Only add non-empty captions\n                    true_captions.append(\" \".join(caption_tokens))\n            \n            # Generate caption for first image in batch\n            generated = model.caption_image(images[0].unsqueeze(0), vocab, device=device)\n            \n            results.append({\n                'image_idx': idx,\n                'true_captions': true_captions[:3],  # First 3 references\n                'generated': generated\n            })\n            \n            print(f\"\\nüì∏ Sample {idx+1}:\")\n            print(f\"  ‚úÖ True: {true_captions[0] if true_captions else 'No caption'}\")\n            print(f\"  ü§ñ Generated: {generated}\")\n    \n    return results\n\n\n# Complete Metrics class\nclass CaptionMetrics:\n    \"\"\"Evaluation metrics for image captioning\"\"\"\n    \n    def __init__(self):\n        self.scores = {}\n    \n    def compute_bleu(self, reference: list, hypothesis: str, n: int = 4) -> float:\n        \"\"\"Compute BLEU-N score\"\"\"\n        if not reference or not hypothesis:\n            return 0.0\n            \n        ref_tokens = [ref.lower().split() for ref in reference]\n        hyp_tokens = hypothesis.lower().split()\n        \n        if len(hyp_tokens) == 0:\n            return 0.0\n        \n        # Brevity penalty\n        ref_lengths = [len(ref) for ref in ref_tokens]\n        closest_ref_len = min(ref_lengths, key=lambda x: abs(x - len(hyp_tokens)))\n        \n        if len(hyp_tokens) < closest_ref_len:\n            bp = np.exp(1 - closest_ref_len / len(hyp_tokens))\n        else:\n            bp = 1.0\n        \n        # N-gram precisions\n        precisions = []\n        for i in range(1, n + 1):\n            hyp_ngrams = self._get_ngrams(hyp_tokens, i)\n            if not hyp_ngrams:\n                precisions.append(0)\n                continue\n                \n            max_ref_counts = Counter()\n            \n            for ref in ref_tokens:\n                ref_ngrams = self._get_ngrams(ref, i)\n                for ngram in ref_ngrams:\n                    max_ref_counts[ngram] = max(max_ref_counts[ngram], ref_ngrams[ngram])\n            \n            clipped_counts = {\n                ngram: min(count, max_ref_counts.get(ngram, 0))\n                for ngram, count in hyp_ngrams.items()\n            }\n            \n            numerator = sum(clipped_counts.values())\n            denominator = max(1, len(hyp_tokens) - i + 1)\n            precisions.append(numerator / denominator if numerator > 0 else 0)\n        \n        # Geometric mean\n        if min(precisions) > 0:\n            geo_mean = np.exp(sum(np.log(p) for p in precisions) / len(precisions))\n        else:\n            geo_mean = 0\n        \n        return bp * geo_mean\n    \n    def _get_ngrams(self, tokens, n):\n        ngrams = []\n        for i in range(len(tokens) - n + 1):\n            ngrams.append(' '.join(tokens[i:i+n]))\n        return Counter(ngrams)\n    \n    def compute_rouge_l(self, reference: list, hypothesis: str) -> float:\n        \"\"\"Compute ROUGE-L score\"\"\"\n        if not reference or not hypothesis:\n            return 0.0\n            \n        hyp_tokens = hypothesis.lower().split()\n        scores = []\n        \n        for ref in reference:\n            ref_tokens = ref.lower().split()\n            lcs_length = self._lcs_length(ref_tokens, hyp_tokens)\n            \n            if len(ref_tokens) == 0 or len(hyp_tokens) == 0:\n                scores.append(0.0)\n                continue\n            \n            precision = lcs_length / len(hyp_tokens)\n            recall = lcs_length / len(ref_tokens)\n            \n            if precision + recall > 0:\n                f1 = 2 * precision * recall / (precision + recall)\n            else:\n                f1 = 0.0\n            \n            scores.append(f1)\n        \n        return max(scores) if scores else 0.0\n    \n    def _lcs_length(self, x, y):\n        m, n = len(x), len(y)\n        dp = [[0] * (n + 1) for _ in range(m + 1)]\n        \n        for i in range(1, m + 1):\n            for j in range(1, n + 1):\n                if x[i-1] == y[j-1]:\n                    dp[i][j] = dp[i-1][j-1] + 1\n                else:\n                    dp[i][j] = max(dp[i-1][j], dp[i][j-1])\n        \n        return dp[m][n]\n    \n    def compute_meteor(self, reference: list, hypothesis: str) -> float:\n        \"\"\"Compute METEOR score\"\"\"\n        if not reference or not hypothesis:\n            return 0.0\n            \n        hyp_tokens = set(hypothesis.lower().split())\n        scores = []\n        \n        for ref in reference:\n            ref_tokens = set(ref.lower().split())\n            \n            if len(hyp_tokens) == 0 and len(ref_tokens) == 0:\n                scores.append(1.0)\n                continue\n            elif len(hyp_tokens) == 0 or len(ref_tokens) == 0:\n                scores.append(0.0)\n                continue\n            \n            matches = len(hyp_tokens & ref_tokens)\n            precision = matches / len(hyp_tokens) if hyp_tokens else 0\n            recall = matches / len(ref_tokens) if ref_tokens else 0\n            \n            if precision + recall > 0:\n                f_mean = (precision * recall) / (0.9 * precision + 0.1 * recall)\n            else:\n                f_mean = 0.0\n            \n            scores.append(f_mean)\n        \n        return max(scores) if scores else 0.0\n    \n    def evaluate_batch(self, references, hypotheses):\n        \"\"\"Evaluate a batch of captions\"\"\"\n        bleu_scores = {1: [], 2: [], 3: [], 4: []}\n        rouge_scores = []\n        meteor_scores = []\n        \n        for img_id in hypotheses:\n            if img_id not in references:\n                continue\n            \n            ref = references[img_id]\n            hyp = hypotheses[img_id]\n            \n            for n in range(1, 5):\n                bleu_scores[n].append(self.compute_bleu(ref, hyp, n=n))\n            \n            rouge_scores.append(self.compute_rouge_l(ref, hyp))\n            meteor_scores.append(self.compute_meteor(ref, hyp))\n        \n        results = {\n            'BLEU-1': np.mean(bleu_scores[1]) if bleu_scores[1] else 0.0,\n            'BLEU-2': np.mean(bleu_scores[2]) if bleu_scores[2] else 0.0,\n            'BLEU-3': np.mean(bleu_scores[3]) if bleu_scores[3] else 0.0,\n            'BLEU-4': np.mean(bleu_scores[4]) if bleu_scores[4] else 0.0,\n            'ROUGE-L': np.mean(rouge_scores) if rouge_scores else 0.0,\n            'METEOR': np.mean(meteor_scores) if meteor_scores else 0.0,\n        }\n        \n        return results\n    \n    def print_evaluation_report(self, results: Dict[str, float]):\n        \"\"\"Print formatted evaluation report\"\"\"\n        print(\"\\n\" + \"=\"*50)\n        print(\"CAPTION GENERATION EVALUATION RESULTS\")\n        print(\"=\"*50)\n        for metric, score in results.items():\n            print(f\"{metric:15s}: {score:.4f}\")\n        print(\"=\"*50 + \"\\n\")\n    \n    def save_results(self, results: Dict[str, float], filepath: str):\n        \"\"\"Save evaluation results to JSON file\"\"\"\n        with open(filepath, 'w') as f:\n            json.dump(results, f, indent=2)\n        print(f\"‚úì Results saved to {filepath}\")\n\n\ndef evaluate_model_comprehensive(model, vocab, test_loader, device=\"cuda\", num_batches=5):\n    \"\"\"Comprehensive evaluation of the model\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"COMPREHENSIVE MODEL EVALUATION\")\n    print(\"=\"*60)\n    \n    evaluator = CaptionMetrics()\n    \n    references = {}\n    hypotheses = {}\n    \n    model.eval()\n    with torch.no_grad():\n        for batch_idx, (images, captions) in enumerate(test_loader):\n            if batch_idx >= num_batches:\n                break\n                \n            images = images.to(device)\n            \n            # Process each image in batch\n            for i in range(min(2, images.size(0))):  # Max 2 images per batch\n                img_id = f\"batch{batch_idx}_img{i}\"\n                \n                # Get true captions\n                true_captions = []\n                for j in range(captions.size(1)):\n                    caption_tokens = []\n                    for token in captions[:, j]:\n                        word = vocab.itos[token.item()]\n                        if word == \"<EOS>\":\n                            break\n                        if word not in [\"<SOS>\", \"<PAD>\"]:\n                            caption_tokens.append(word)\n                    if caption_tokens:\n                        true_captions.append(\" \".join(caption_tokens))\n                \n                if true_captions:\n                    references[img_id] = true_captions\n                    \n                    # Generate caption\n                    generated = model.caption_image(\n                        images[i].unsqueeze(0), \n                        vocab, \n                        device=device,\n                        max_length=20\n                    )\n                    hypotheses[img_id] = generated\n    \n    # Calculate metrics\n    if references and hypotheses:\n        results = evaluator.evaluate_batch(references, hypotheses)\n        \n        # Print report\n        evaluator.print_evaluation_report(results)\n        \n        # Save detailed results\n        detailed_results = {\n            \"metrics\": results,\n            \"sample_count\": len(hypotheses),\n            \"samples\": []\n        }\n        \n        # Add sample predictions\n        for img_id, hyp in list(hypotheses.items())[:3]:\n            detailed_results[\"samples\"].append({\n                \"image_id\": img_id,\n                \"generated\": hyp,\n                \"references\": references.get(img_id, [])\n            })\n        \n        results_file = \"/kaggle/working/evaluation_results_detailed.json\"\n        with open(results_file, 'w') as f:\n            json.dump(detailed_results, f, indent=2)\n        \n        print(f\"‚úì Detailed results saved to {results_file}\")\n        \n        return results\n    else:\n        print(\"‚ùå No data to evaluate\")\n        return None\n\n\ndef run_complete_evaluation_pipeline():\n    \"\"\"Run complete evaluation pipeline\"\"\"\n    print(\"Running complete evaluation pipeline...\")\n    \n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Device: {device}\")\n    \n    # Check for trained models\n    model_files = [f for f in os.listdir(\"/kaggle/working\") \n                  if f.endswith(\".pth\") and \"best_model\" in f]\n    \n    if not model_files:\n        model_files = [f for f in os.listdir(\"/kaggle/working\") \n                      if f.endswith(\".pth\")]\n    \n    if model_files:\n        # Load the best or latest model\n        model_path = f\"/kaggle/working/{sorted(model_files)[-1]}\"\n        print(f\"Loading model: {os.path.basename(model_path)}\")\n        \n        model, vocab = load_trained_model(model_path, device)\n        \n        # Get test data loader\n        transform = transforms.Compose([\n            transforms.Resize((299, 299)),\n            transforms.ToTensor(),\n            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n        ])\n        \n        _, _, test_loader, _ = get_loaders(\n            transform=transform,\n            batch_size=8,\n            num_workers=0,\n            shuffle=False\n        )\n        \n        # 1. Generate sample captions\n        print(\"\\n\" + \"=\"*60)\n        print(\"GENERATING SAMPLE CAPTIONS\")\n        print(\"=\"*60)\n        samples = generate_captions_for_test_set(\n            model, vocab, test_loader, device, num_samples=3\n        )\n        \n        # 2. Comprehensive evaluation\n        print(\"\\n\" + \"=\"*60)\n        print(\"RUNNING COMPREHENSIVE EVALUATION\")\n        print(\"=\"*60)\n        results = evaluate_model_comprehensive(\n            model, vocab, test_loader, device, num_batches=3\n        )\n        \n        # 3. Save final summary\n        if results:\n            summary = {\n                \"model\": os.path.basename(model_path),\n                \"evaluation_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n                \"metrics\": results,\n                \"sample_predictions\": samples\n            }\n            \n            summary_file = \"/kaggle/working/final_evaluation_summary.json\"\n            with open(summary_file, 'w') as f:\n                json.dump(summary, f, indent=2)\n            \n            print(f\"\\n‚úì Final evaluation summary saved to {summary_file}\")\n        \n        return model, vocab, results\n    else:\n        print(\"‚ùå No trained models found. Please train a model first.\")\n        return None, None, None\n\n\n# Example usage demonstration\nif __name__ == \"__main__\":\n    print(\"Testing evaluation pipeline...\")\n    \n    # Create a simple test if no trained model exists\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    # Test metrics class\n    print(\"\\nTesting metrics class with sample data...\")\n    evaluator = CaptionMetrics()\n    \n    # Sample data\n    test_references = {\n        \"img1\": [\"a dog playing with a ball\", \"a brown dog playing with a red ball\"],\n        \"img2\": [\"a person riding a bicycle\", \"someone cycling on the road\"]\n    }\n    \n    test_hypotheses = {\n        \"img1\": \"a dog playing with ball\",\n        \"img2\": \"a person riding a bike\"\n    }\n    \n    test_results = evaluator.evaluate_batch(test_references, test_hypotheses)\n    evaluator.print_evaluation_report(test_results)\n    \n    print(\"‚úì Metrics class test successful!\")\n    print(\"\\nTo run full evaluation on your trained model:\")\n    print(\"1. Train a model using Cell 5 or Cell 8\")\n    print(\"2. Run: model, vocab, results = run_complete_evaluation_pipeline()\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T15:54:22.543800Z","iopub.execute_input":"2026-01-19T15:54:22.544335Z","iopub.status.idle":"2026-01-19T15:54:22.585214Z","shell.execute_reply.started":"2026-01-19T15:54:22.544304Z","shell.execute_reply":"2026-01-19T15:54:22.584656Z"}},"outputs":[{"name":"stdout","text":"Testing evaluation pipeline...\n\nTesting metrics class with sample data...\n\n==================================================\nCAPTION GENERATION EVALUATION RESULTS\n==================================================\nBLEU-1         : 0.8094\nBLEU-2         : 0.7418\nBLEU-3         : 0.6933\nBLEU-4         : 0.6238\nROUGE-L        : 0.8545\nMETEOR         : 0.8750\n==================================================\n\n‚úì Metrics class test successful!\n\nTo run full evaluation on your trained model:\n1. Train a model using Cell 5 or Cell 8\n2. Run: model, vocab, results = run_complete_evaluation_pipeline()\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"# ============================================================================\n# CELL 7: UTILITIES & HELPER FUNCTIONS\n# ============================================================================\n\ndef save_checkpoint(model, optimizer, epoch, val_loss, vocab, config, filename):\n    \"\"\"Save model checkpoint\"\"\"\n    checkpoint = {\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'val_loss': val_loss,\n        'vocab': vocab,\n        'config': config\n    }\n    torch.save(checkpoint, filename)\n    print(f\"‚úì Checkpoint saved: {filename}\")\n\n\ndef load_checkpoint(filename, device=\"cuda\"):\n    \"\"\"Load model checkpoint\"\"\"\n    checkpoint = torch.load(filename, map_location=device)\n    return checkpoint\n\n\ndef cleanup_memory():\n    \"\"\"Clean up GPU memory (important for Kaggle)\"\"\"\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    import gc\n    gc.collect()\n\n\ndef get_model_summary(model):\n    \"\"\"Print model summary\"\"\"\n    total_params = sum(p.numel() for p in model.parameters())\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    \n    print(\"=\"*60)\n    print(\"MODEL SUMMARY\")\n    print(\"=\"*60)\n    print(f\"Total parameters: {total_params:,}\")\n    print(f\"Trainable parameters: {trainable_params:,}\")\n    print(f\"Model size: {total_params * 4 / 1024**2:.2f} MB (float32)\")\n    print(\"=\"*60)\n    \n    return total_params, trainable_params\n\n\n# Test the training with fixed scheduler\nif __name__ == \"__main__\":\n    # Clean up any existing memory\n    cleanup_memory()\n    \n    # Run training with fixed scheduler\n    print(\"Testing training with fixed scheduler...\")\n    \n    # Quick test with 1 epoch first\n    try:\n        model, vocab = train_model(\n            num_epochs=1,  # Just 1 epoch for testing\n            batch_size=16,  # Smaller batch for testing\n            learning_rate=3e-4,\n            embed_size=256,\n            hidden_size=256,\n            num_layers=1,\n            grad_clip=5.0,\n            save_every=1\n        )\n        print(\"\\n‚úì Training test successful!\")\n        \n        # Show model summary\n        get_model_summary(model)\n        \n    except Exception as e:\n        print(f\"\\n‚úó Error during training: {e}\")\n        print(\"\\nTrying alternative approach...\")\n        \n        # Alternative: Train without scheduler first\n        print(\"\\nStarting training without scheduler...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T15:55:11.701069Z","iopub.execute_input":"2026-01-19T15:55:11.701712Z","iopub.status.idle":"2026-01-19T16:13:28.388168Z","shell.execute_reply.started":"2026-01-19T15:55:11.701687Z","shell.execute_reply":"2026-01-19T16:13:28.387260Z"}},"outputs":[{"name":"stdout","text":"Testing training with fixed scheduler...\n\n============================================================\nSTARTING TRAINING ON KAGGLE\n============================================================\nUsing device: cuda\nGPU: Tesla T4\nGPU Memory: 15.83 GB\n[Experiment] Initialized: /kaggle/working/experiments/2026-01-19_15-55-11_ResNet101_LSTM_v1\n[Experiment] Config saved.\n\nInitializing Loaders...\nLoading data from: /kaggle/input/flickr8kimagescaptions/flickr8k/captions.txt\nLoaded 40455 image-caption pairs\nSplit: Train=32365, Val=4045, Test=4045\nVocabulary size: 2732\n\nStarting training for 1 epochs...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2023/2023 [17:32<00:00,  1.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 001 | Train Loss: 4.1951 | Val Loss: 3.6272 | LR: 0.000300\nTime: 1094.1s\n‚úì Saved best model (val_loss: 3.6272)\n\n============================================================\nTRAINING COMPLETE\nBest validation loss: 3.6272\nModels saved in: /kaggle/working/models\n============================================================\n\n‚úì Training test successful!\n============================================================\nMODEL SUMMARY\n============================================================\nTotal parameters: 45,184,432\nTrainable parameters: 45,184,432\nModel size: 172.36 MB (float32)\n============================================================\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"# ============================================================================\n# CELL 8: ALTERNATIVE TRAINING FUNCTION (FIXED)\n# ============================================================================\n\ndef train_simple_model(num_epochs=3):\n    \"\"\"Simplified training function for Kaggle\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"SIMPLE TRAINING (No Scheduler)\")\n    print(\"=\"*60)\n    \n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Device: {device}\")\n    \n    # Data transforms\n    transform = transforms.Compose([\n        transforms.Resize((299, 299)),\n        transforms.ToTensor(),\n        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n    ])\n    \n    # Get data loaders (smaller for testing)\n    print(\"\\nLoading data...\")\n    train_loader, val_loader, test_loader, vocab = get_loaders(\n        transform=transform,\n        batch_size=16,\n        num_workers=2,\n        shuffle=True,\n        pin_memory=True,\n        test_size=0.05,  # Smaller test set\n        val_size=0.05    # Smaller validation set\n    )\n    \n    print(f\"Vocabulary size: {len(vocab)}\")\n    \n    # Create simple model\n    model = CNNtoRNN(\n        embed_size=256,\n        hidden_size=256,\n        vocab_size=len(vocab),\n        num_layers=1\n    ).to(device)\n    \n    # Loss and optimizer (no scheduler)\n    criterion = nn.CrossEntropyLoss(ignore_index=vocab.stoi[\"<PAD>\"])\n    optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n    \n    print(\"\\nStarting training...\")\n    \n    for epoch in range(num_epochs):\n        model.train()\n        total_loss = 0\n        batch_count = 0\n        \n        # Training loop\n        for images, captions in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n            images = images.to(device)\n            captions = captions.to(device)\n            \n            # Forward\n            outputs = model(images, captions[:-1])\n            loss = criterion(\n                outputs.reshape(-1, outputs.shape[2]),\n                captions[1:].reshape(-1)\n            )\n            \n            # Backward\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n            optimizer.step()\n            \n            total_loss += loss.item()\n            batch_count += 1\n            \n            # Memory cleanup every 10 batches\n            if batch_count % 10 == 0 and device.type == \"cuda\":\n                torch.cuda.empty_cache()\n        \n        avg_loss = total_loss / batch_count\n        print(f\"Epoch {epoch+1} - Average Loss: {avg_loss:.4f}\")\n        \n        # Save checkpoint\n        if (epoch + 1) % 2 == 0:\n            checkpoint_path = f\"/kaggle/working/model_epoch_{epoch+1}.pth\"\n            save_checkpoint(\n                model, optimizer, epoch, avg_loss, vocab,\n                {\"epoch\": epoch, \"loss\": avg_loss},\n                checkpoint_path\n            )\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"TRAINING COMPLETE\")\n    print(\"=\"*60)\n    \n    return model, vocab\n\n\n# Run simple training\nif __name__ == \"__main__\":\n    print(\"Running simplified training...\")\n    model, vocab = train_simple_model(num_epochs=2)\n    \n    # Test inference\n    print(\"\\nTesting inference on sample images...\")\n    \n    # Get a sample batch\n    transform = transforms.Compose([\n        transforms.Resize((299, 299)),\n        transforms.ToTensor(),\n        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n    ])\n    \n    train_loader, val_loader, test_loader, _ = get_loaders(\n        transform=transform,\n        batch_size=4,\n        num_workers=0,\n        shuffle=False\n    )\n    \n    # Generate caption for first image\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.eval()\n    \n    with torch.no_grad():\n        for images, captions in test_loader:\n            images = images.to(device)\n            \n            # Generate caption for first image\n            caption = model.caption_image(images[0].unsqueeze(0), vocab, device=device)\n            \n            # Get true caption\n            true_caption_tokens = []\n            for token in captions[:, 0]:\n                word = vocab.itos[token.item()]\n                if word == \"<EOS>\":\n                    break\n                if word not in [\"<SOS>\", \"<PAD>\"]:\n                    true_caption_tokens.append(word)\n            true_caption = \" \".join(true_caption_tokens)\n            \n            print(f\"\\nGenerated Caption: {caption}\")\n            print(f\"True Caption: {true_caption}\")\n            break\n    \n    print(\"\\n‚úì Inference test successful!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T16:14:11.051653Z","iopub.execute_input":"2026-01-19T16:14:11.052362Z","iopub.status.idle":"2026-01-19T16:54:01.517217Z","shell.execute_reply.started":"2026-01-19T16:14:11.052312Z","shell.execute_reply":"2026-01-19T16:54:01.516483Z"}},"outputs":[{"name":"stdout","text":"Running simplified training...\n\n============================================================\nSIMPLE TRAINING (No Scheduler)\n============================================================\nDevice: cuda\n\nLoading data...\nLoading data from: /kaggle/input/flickr8kimagescaptions/flickr8k/captions.txt\nLoaded 40455 image-caption pairs\nSplit: Train=36415, Val=2020, Test=2020\nVocabulary size: 2882\nVocabulary size: 2882\n\nStarting training...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2276/2276 [19:55<00:00,  1.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 - Average Loss: 4.1536\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2276/2276 [19:52<00:00,  1.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 - Average Loss: 3.5728\n‚úì Checkpoint saved: /kaggle/working/model_epoch_2.pth\n\n============================================================\nTRAINING COMPLETE\n============================================================\n\nTesting inference on sample images...\nLoading data from: /kaggle/input/flickr8kimagescaptions/flickr8k/captions.txt\nLoaded 40455 image-caption pairs\nSplit: Train=32365, Val=4045, Test=4045\nVocabulary size: 2732\n\nGenerated Caption: a man in a blue shirt is standing on a <UNK> .\nTrue Caption: a ice lined <UNK> people baby a throwing booths .\n\n‚úì Inference test successful!\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"# ============================================================================\n# CELL 9: EVALUATION & METRICS COMPLETE - FIXED\n# ============================================================================\n\ndef load_checkpoint(filename, device=\"cuda\"):\n    \"\"\"Load model checkpoint - FIXED for PyTorch 2.6\"\"\"\n    try:\n        # Method 1: Try with weights_only=False (less secure but works for trusted checkpoints)\n        checkpoint = torch.load(filename, map_location=device, weights_only=False)\n    except Exception as e:\n        print(f\"First load attempt failed: {e}\")\n        try:\n            # Method 2: Add safe globals for Vocabulary class\n            import torch.serialization\n            torch.serialization.add_safe_globals([Vocabulary])\n            checkpoint = torch.load(filename, map_location=device, weights_only=True)\n        except Exception as e2:\n            print(f\"Second load attempt failed: {e2}\")\n            # Method 3: Last resort - load without weights_only (backward compatible)\n            checkpoint = torch.load(filename, map_location=device)\n    \n    return checkpoint\n\n\ndef evaluate_model(model, test_loader, vocab, device=\"cuda\", num_samples=10):\n    \"\"\"Evaluate model on test set\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"MODEL EVALUATION\")\n    print(\"=\"*60)\n    \n    model.eval()\n    evaluator = CaptionMetrics()\n    \n    references = {}\n    hypotheses = {}\n    \n    with torch.no_grad():\n        for batch_idx, (images, captions) in enumerate(tqdm(test_loader, desc=\"Evaluating\")):\n            if batch_idx >= 5:  # Limit to 5 batches for speed\n                break\n                \n            images = images.to(device)\n            \n            # Process each image in batch\n            for i in range(images.size(0)):\n                img_id = f\"img_{batch_idx}_{i}\"\n                \n                # Get true captions (all references for this image)\n                true_captions = []\n                for j in range(captions.size(1)):\n                    caption_tokens = []\n                    for token in captions[:, j]:\n                        word = vocab.itos[token.item()]\n                        if word == \"<EOS>\":\n                            break\n                        if word not in [\"<SOS>\", \"<PAD>\"]:\n                            caption_tokens.append(word)\n                    true_captions.append(\" \".join(caption_tokens))\n                \n                references[img_id] = true_captions\n                \n                # Generate caption\n                generated = model.caption_image(\n                    images[i].unsqueeze(0), \n                    vocab, \n                    device=device,\n                    max_length=20\n                )\n                hypotheses[img_id] = generated\n    \n    # Calculate metrics\n    if references and hypotheses:\n        results = evaluator.evaluate_batch(references, hypotheses)\n        \n        print(\"\\nEVALUATION RESULTS:\")\n        print(\"=\"*40)\n        for metric, score in results.items():\n            print(f\"{metric:15s}: {score:.4f}\")\n        print(\"=\"*40)\n        \n        # Save results\n        results_file = \"/kaggle/working/evaluation_results.json\"\n        import json\n        with open(results_file, 'w') as f:\n            json.dump({\n                \"metrics\": results,\n                \"sample_count\": len(hypotheses),\n                \"references_sample\": {k: v[:2] for k, v in list(references.items())[:2]},\n                \"hypotheses_sample\": {k: v for k, v in list(hypotheses.items())[:2]}\n            }, f, indent=2)\n        \n        print(f\"\\n‚úì Results saved to: {results_file}\")\n        \n        return results\n    else:\n        print(\"No data to evaluate\")\n        return None\n\n\ndef interactive_inference(model, vocab, device=\"cuda\"):\n    \"\"\"Interactive inference on test images\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"INTERACTIVE INFERENCE\")\n    print(\"=\"*60)\n    \n    # Get test loader\n    transform = transforms.Compose([\n        transforms.Resize((299, 299)),\n        transforms.ToTensor(),\n        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n    ])\n    \n    _, _, test_loader, _ = get_loaders(\n        transform=transform,\n        batch_size=4,\n        num_workers=0,\n        shuffle=True\n    )\n    \n    model.eval()\n    \n    # Generate captions for a few images\n    print(\"\\nGenerating captions for sample images...\\n\")\n    \n    with torch.no_grad():\n        for batch_idx, (images, captions) in enumerate(test_loader):\n            if batch_idx >= 3:  # Show 3 batches\n                break\n                \n            images = images.to(device)\n            \n            print(f\"\\n{'='*40}\")\n            print(f\"BATCH {batch_idx + 1}\")\n            print('='*40)\n            \n            for i in range(min(2, images.size(0))):  # Show 2 images per batch\n                # Generate caption\n                generated = model.caption_image(\n                    images[i].unsqueeze(0), \n                    vocab, \n                    device=device\n                )\n                \n                # Get true captions\n                true_captions = []\n                for j in range(3):  # Show 3 true captions\n                    caption_tokens = []\n                    for token in captions[:, j]:\n                        word = vocab.itos[token.item()]\n                        if word == \"<EOS>\":\n                            break\n                        if word not in [\"<SOS>\", \"<PAD>\"]:\n                            caption_tokens.append(word)\n                    if caption_tokens:\n                        true_captions.append(\" \".join(caption_tokens))\n                \n                print(f\"\\nImage {i+1}:\")\n                print(f\"  Generated: {generated}\")\n                print(f\"  True captions:\")\n                for idx, true_cap in enumerate(true_captions[:2]):\n                    print(f\"    {idx+1}. {true_cap}\")\n                print()\n\n\n# FIXED: Safe model loading function\ndef load_trained_model_safe(model_path, device=\"cuda\"):\n    \"\"\"Load trained model safely for PyTorch 2.6\"\"\"\n    print(f\"Loading model: {os.path.basename(model_path)}\")\n    \n    # Load checkpoint with multiple fallback methods\n    checkpoint = load_checkpoint(model_path, device)\n    \n    # Recreate vocabulary from saved dictionaries\n    if 'vocab' in checkpoint:\n        vocab = checkpoint['vocab']\n    else:\n        # Fallback: recreate vocabulary from saved dicts\n        vocab = Vocabulary(freq_threshold=5)\n        vocab.stoi = checkpoint.get('vocab_stoi', {})\n        vocab.itos = {int(k): v for k, v in checkpoint.get('vocab_itos', {}).items()}\n    \n    # Get config with defaults\n    config = checkpoint.get('config', {\n        'embed_size': 256,\n        'hidden_size': 256,\n        'num_layers': 1\n    })\n    \n    # Create model\n    model = CNNtoRNN(\n        embed_size=config.get('embed_size', 256),\n        hidden_size=config.get('hidden_size', 256),\n        vocab_size=len(vocab),\n        num_layers=config.get('num_layers', 1)\n    ).to(device)\n    \n    # Load weights\n    model.load_state_dict(checkpoint['model_state_dict'])\n    model.eval()\n    \n    print(f\"‚úì Model loaded from epoch {checkpoint.get('epoch', 'unknown')}\")\n    print(f\"  Validation loss: {checkpoint.get('val_loss', 'unknown'):.4f}\")\n    print(f\"  Vocabulary size: {len(vocab)}\")\n    \n    return model, vocab\n\n\n# Main execution - FIXED\nif __name__ == \"__main__\":\n    print(\"Starting evaluation pipeline...\")\n    \n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n    \n    # Find trained models\n    model_files = [f for f in os.listdir(\"/kaggle/working\") \n                  if f.endswith(\".pth\") and not f.startswith(\".\")]\n    \n    if model_files:\n        latest_model = sorted(model_files)[-1]\n        model_path = f\"/kaggle/working/{latest_model}\"\n        \n        try:\n            # Load model using safe method\n            model, vocab = load_trained_model_safe(model_path, device)\n            \n            # Prepare test loader\n            transform = transforms.Compose([\n                transforms.Resize((299, 299)),\n                transforms.ToTensor(),\n                transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n            ])\n            \n            _, _, test_loader, _ = get_loaders(\n                transform=transform,\n                batch_size=8,\n                num_workers=0,\n                shuffle=False\n            )\n            \n            # Quick test first\n            print(\"\\n\" + \"=\"*60)\n            print(\"QUICK MODEL TEST\")\n            print(\"=\"*60)\n            \n            # Test on one batch\n            for images, captions in test_loader:\n                images = images.to(device)\n                caption = model.caption_image(images[0].unsqueeze(0), vocab, device=device)\n                \n                # Get true caption\n                true_words = []\n                for token in captions[:, 0]:\n                    word = vocab.itos[token.item()]\n                    if word == \"<EOS>\":\n                        break\n                    if word not in [\"<SOS>\", \"<PAD>\"]:\n                        true_words.append(word)\n                \n                print(f\"\\n‚úÖ Model works!\")\n                print(f\"   Generated: {caption}\")\n                print(f\"   True: {' '.join(true_words)}\")\n                break\n            \n            # Ask user if they want full evaluation\n            print(\"\\n\" + \"=\"*60)\n            response = input(\"Run full evaluation? (y/n): \").lower().strip()\n            \n            if response == 'y':\n                # Run evaluation\n                results = evaluate_model(model, test_loader, vocab, device, num_samples=10)\n                \n                # Interactive inference\n                interactive_inference(model, vocab, device)\n            else:\n                print(\"Skipping full evaluation. Model is loaded and ready for use.\")\n                \n        except Exception as e:\n            print(f\"‚ùå Error loading model: {e}\")\n            print(\"\\nTrying alternative loading method...\")\n            \n            # Try direct loading\n            try:\n                checkpoint = torch.load(model_path, map_location=device, weights_only=False)\n                print(\"‚úì Loaded with weights_only=False\")\n                \n                # Show what's in checkpoint\n                print(f\"Checkpoint keys: {list(checkpoint.keys())}\")\n                \n            except Exception as e2:\n                print(f\"Still failing: {e2}\")\n                print(\"\\nPlease try the Quick Evaluation Workaround cell instead.\")\n                \n    else:\n        print(\"‚ùå No trained models found.\")\n        print(\"Check /kaggle/working/ for .pth files\")\n        print(\"If you just trained, list files with: !ls -la /kaggle/working/\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T17:20:52.503429Z","iopub.execute_input":"2026-01-19T17:20:52.504115Z","iopub.status.idle":"2026-01-19T17:21:35.943522Z","shell.execute_reply.started":"2026-01-19T17:20:52.504088Z","shell.execute_reply":"2026-01-19T17:21:35.942740Z"}},"outputs":[{"name":"stdout","text":"Starting evaluation pipeline...\nUsing device: cuda\nLoading model: model_epoch_2.pth\nFirst load attempt failed: cannot access local variable 'torch' where it is not associated with a value\n‚úì Model loaded from epoch 1\n  Validation loss: 3.5728\n  Vocabulary size: 2882\nLoading data from: /kaggle/input/flickr8kimagescaptions/flickr8k/captions.txt\nLoaded 40455 image-caption pairs\nSplit: Train=32365, Val=4045, Test=4045\nVocabulary size: 2732\n\n============================================================\nQUICK MODEL TEST\n============================================================\n\n‚úÖ Model works!\n   Generated: a man in a blue shirt is standing on a <UNK> .\n   True: a ice lined <UNK> people baby a throwing booths .\n\n============================================================\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Run full evaluation? (y/n):  y\n"},{"name":"stdout","text":"\n============================================================\nMODEL EVALUATION\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:   1%|          | 5/506 [00:01<02:15,  3.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEVALUATION RESULTS:\n========================================\nBLEU-1         : 0.4394\nBLEU-2         : 0.1137\nBLEU-3         : 0.0000\nBLEU-4         : 0.0000\nROUGE-L        : 0.3238\nMETEOR         : 0.3163\n========================================\n\n‚úì Results saved to: /kaggle/working/evaluation_results.json\n\n============================================================\nINTERACTIVE INFERENCE\n============================================================\nLoading data from: /kaggle/input/flickr8kimagescaptions/flickr8k/captions.txt\nLoaded 40455 image-caption pairs\nSplit: Train=32365, Val=4045, Test=4045\nVocabulary size: 2732\n\nGenerating captions for sample images...\n\n\n========================================\nBATCH 1\n========================================\n\nImage 1:\n  Generated: a man in a blue shirt is standing on a <UNK> .\n  True captions:\n    1. a ice lined <UNK> people baby a throwing booths .\n    2. a ice group of and display hat lined girl white little a booths .\n\n\nImage 2:\n  Generated: a man in a blue shirt is standing on a <UNK> .\n  True captions:\n    1. a ice lined <UNK> people baby a throwing booths .\n    2. a ice group of and display hat lined girl white little a booths .\n\n\n========================================\nBATCH 2\n========================================\n\nImage 1:\n  Generated: a man in a blue shirt is standing on a <UNK> .\n  True captions:\n    1. away bench camera girl white little and elephant reading talking .\n    2. a outfit hat a with the balls into looking of a lot .\n\n\nImage 2:\n  Generated: a man in a red shirt is standing on a <UNK> .\n  True captions:\n    1. away bench camera girl white little and elephant reading talking .\n    2. a outfit hat a with the balls into looking of a lot .\n\n\n========================================\nBATCH 3\n========================================\n\nImage 1:\n  Generated: a man in a red shirt is standing on a <UNK> .\n  True captions:\n    1. a with hat grassy boy of a motion water an a sitting .\n    2. a glasses with hat a grassy boy of a lot .\n\n\nImage 2:\n  Generated: a man in a red shirt is standing on a <UNK> .\n  True captions:\n    1. a with hat grassy boy of a motion water an a sitting .\n    2. a glasses with hat a grassy boy of a lot .\n\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"# ============================================================================\n# CELL 10: FINAL SUMMARY & EXPORT\n# ============================================================================\n\ndef create_final_summary():\n    \"\"\"Create final summary of the experiment\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"EXPERIMENT SUMMARY\")\n    print(\"=\"*60)\n    \n    # Check what we have\n    print(\"\\nüìÅ Output Directory Contents:\")\n    !ls -la /kaggle/working/\n    \n    print(\"\\nüìä Experiment Logs:\")\n    if os.path.exists(\"/kaggle/working/experiments\"):\n        experiments = os.listdir(\"/kaggle/working/experiments\")\n        for exp in experiments[:3]:  # Show first 3\n            print(f\"  - {exp}\")\n    \n    print(\"\\nü§ñ Trained Models:\")\n    model_files = [f for f in os.listdir(\"/kaggle/working\") if f.endswith(\".pth\")]\n    for model_file in model_files[:5]:  # Show first 5\n        print(f\"  - {model_file}\")\n    \n    # Dataset info\n    print(\"\\nüì¶ Dataset Information:\")\n    if os.path.exists(ANNOTATION_FILE):\n        with open(ANNOTATION_FILE, 'r') as f:\n            lines = f.readlines()\n            print(f\"  Captions file: {len(lines)} lines\")\n    \n    if os.path.exists(IMAGE_DIR):\n        images = [f for f in os.listdir(IMAGE_DIR) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n        print(f\"  Images directory: {len(images)} images\")\n    \n    # GPU info\n    if torch.cuda.is_available():\n        print(f\"\\n‚ö° GPU: {torch.cuda.get_device_name(0)}\")\n        print(f\"  Memory used: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n        print(f\"  Memory cached: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"NEXT STEPS:\")\n    print(\"=\"*60)\n    print(\"1. To train longer: Increase num_epochs in train_simple_model()\")\n    print(\"2. To save outputs: Download files from /kaggle/working/\")\n    print(\"3. To improve: Adjust batch_size, learning_rate, model size\")\n    print(\"4. To evaluate: Run the evaluation cell after training\")\n    print(\"=\"*60)\n\n\n# Create summary\ncreate_final_summary()\n\nprint(\"\\n‚úÖ Kaggle Notebook Setup Complete!\")\nprint(\"\\nTo run the full pipeline:\")\nprint(\"1. Run Cell 1-4 for setup\")\nprint(\"2. Run Cell 8 for training\")\nprint(\"3. Run Cell 9 for evaluation\")\nprint(\"4. Run Cell 10 for summary\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T17:25:18.642192Z","iopub.execute_input":"2026-01-19T17:25:18.643041Z","iopub.status.idle":"2026-01-19T17:25:18.874356Z","shell.execute_reply.started":"2026-01-19T17:25:18.643012Z","shell.execute_reply":"2026-01-19T17:25:18.873390Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nEXPERIMENT SUMMARY\n============================================================\n\nüìÅ Output Directory Contents:\ntotal 531412\ndrwxr-xr-x 6 root root      4096 Jan 19 17:21 .\ndrwxr-xr-x 5 root root      4096 Jan 19 14:31 ..\ndrwxr-xr-x 5 root root      4096 Jan 19 14:44 Deep_Learning_final\n-rw-r--r-- 1 root root       721 Jan 19 17:21 evaluation_results.json\ndrwxr-xr-x 6 root root      4096 Jan 19 15:55 experiments\n-rw-r--r-- 1 root root 544130280 Jan 19 16:54 model_epoch_2.pth\ndrwxr-xr-x 2 root root      4096 Jan 19 16:13 models\ndrwxr-xr-x 2 root root      4096 Jan 19 14:31 .virtual_documents\n\nüìä Experiment Logs:\n  - 2026-01-19_14-53-10_ResNet101_LSTM_v1\n  - 2026-01-19_15-55-11_ResNet101_LSTM_v1\n  - 2026-01-19_14-57-03_ResNet101_LSTM_v1\n\nü§ñ Trained Models:\n  - model_epoch_2.pth\n\nüì¶ Dataset Information:\n  Captions file: 40456 lines\n  Images directory: 8091 images\n\n‚ö° GPU: Tesla T4\n  Memory used: 0.40 GB\n  Memory cached: 5.73 GB\n\n============================================================\nNEXT STEPS:\n============================================================\n1. To train longer: Increase num_epochs in train_simple_model()\n2. To save outputs: Download files from /kaggle/working/\n3. To improve: Adjust batch_size, learning_rate, model size\n4. To evaluate: Run the evaluation cell after training\n============================================================\n\n‚úÖ Kaggle Notebook Setup Complete!\n\nTo run the full pipeline:\n1. Run Cell 1-4 for setup\n2. Run Cell 8 for training\n3. Run Cell 9 for evaluation\n4. Run Cell 10 for summary\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}