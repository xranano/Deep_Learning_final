# The Visual Storyteller - Image Captioning Project

A deep learning system that generates natural language descriptions from images using state-of-the-art neural architectures.

## ğŸ“‹ Project Overview

This project implements an **image captioning system** that bridges computer vision and natural language processing. The model takes an image as input and generates a descriptive caption, demonstrating the power of sequence-to-sequence learning with attention mechanisms.

### Key Features

- ğŸ–¼ï¸ **End-to-end image captioning** using encoder-decoder architecture
- ğŸ¯ **Attention mechanism** to focus on relevant image regions
- ğŸ“Š **Comprehensive evaluation metrics** (BLEU, METEOR, ROUGE-L, CIDEr)
- ğŸ” **Attention visualization** tools
- ğŸ”„ **Data augmentation** for improved model robustness
- ğŸ“ˆ **Training monitoring** with loss curves and validation metrics

## ğŸ—ï¸ Architecture

The model consists of two main components:

1. **Image Encoder**: Convolutional Neural Network (ResNet/VGG/EfficientNet) that extracts visual features
2. **Caption Decoder**: LSTM/GRU with attention mechanism that generates word sequences

```
Image â†’ CNN Encoder â†’ Visual Features â†’ Attention Decoder â†’ Caption
```

## ğŸ“ Project Structure *áƒ‘áƒáƒšáƒáƒ¡ áƒáƒ¡áƒ”áƒ— áƒ¡áƒ¢áƒ áƒ£áƒ¥áƒ¢áƒ£áƒ áƒáƒ“ áƒ’áƒáƒ“áƒáƒ•áƒáƒ™áƒ”áƒ—áƒáƒ—*

```
Deep_Learning_final/
â”œâ”€â”€ data_loader.py              # Data loading and preprocessing
â”œâ”€â”€ model.py                    # Model architecture definition
â”œâ”€â”€ train.py                    # Training script
â”œâ”€â”€ inference.py                # Caption generation
â”œâ”€â”€ utils.py                    # Helper functions
â”œâ”€â”€ evaluation_metrics.py       # Metrics calculation (BLEU, METEOR, etc.)
â”œâ”€â”€ attention_visualization.py  # Attention visualization tools
â”œâ”€â”€ data_augmentation.py        # Image and caption augmentation
â”œâ”€â”€ verification.py             # Model verification
â”œâ”€â”€ check_setup.py              # Environment setup checker
â”œâ”€â”€ experiments/                # Experimental notebooks
â””â”€â”€ README.md                   # This file
```

## ğŸš€ Getting Started

### Prerequisites

```bash
Python 3.8+
PyTorch 2.0+
torchvision
numpy
matplotlib
PIL
scipy
seaborn
```

### Installation

1. Clone the repository:
```bash
git clone https://github.com/xranano/Deep_Learning_final.git
cd Deep_Learning_final
```

2. Install dependencies:
```bash
pip install torch torchvision numpy matplotlib pillow scipy seaborn
```

3. Verify setup:
```bash
python check_setup.py
```

### Dataset Preparation

1. Download the dataset (caption_data.zip)
2. Extract to `data/` directory
3. The dataset should contain:
   - 8,000 unique images
   - 5 captions per image (40,000 total captions)

Expected structure:
```
data/
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ image_001.jpg
â”‚   â”œâ”€â”€ image_002.jpg
â”‚   â””â”€â”€ ...
â””â”€â”€ captions.txt
```

## ğŸ“ Training

### Basic Training

```bash
python train.py --epochs 50 --batch_size 32 --learning_rate 0.001
```

### Training with Custom Configuration

```bash
python train.py \
    --epochs 100 \
    --batch_size 64 \
    --learning_rate 0.0001 \
    --encoder resnet50 \
    --decoder_dim 512 \
    --attention_dim 512 \
    --embed_dim 512 \
    --dropout 0.5 \
    --save_dir ./checkpoints
```

### Parameters

- `--epochs`: Number of training epochs (default: 50)
- `--batch_size`: Batch size (default: 32)
- `--learning_rate`: Learning rate (default: 0.001)
- `--encoder`: CNN encoder (resnet50, resnet101, vgg16, efficientnet)
- `--decoder_dim`: LSTM hidden dimension (default: 512)
- `--attention_dim`: Attention layer dimension (default: 512)
- `--embed_dim`: Word embedding dimension (default: 512)
- `--dropout`: Dropout rate (default: 0.5)

## ğŸ”® Inference

### Generate Caption for Single Image

```python
from inference import generate_caption
from model import ImageCaptioningModel

# Load trained model
model = ImageCaptioningModel.load('checkpoints/best_model.pth')

# Generate caption
caption = generate_caption('path/to/image.jpg', model)
print(f"Generated Caption: {caption}")
```

### Batch Inference

```python
from inference import batch_inference

images = ['img1.jpg', 'img2.jpg', 'img3.jpg']
captions = batch_inference(images, model)

for img, cap in zip(images, captions):
    print(f"{img}: {cap}")
```

## ğŸ“Š Evaluation

### Compute Metrics

```python
from evaluation_metrics import CaptionMetrics

evaluator = CaptionMetrics()

# Evaluate on test set
results = evaluator.evaluate_batch(references, hypotheses)
evaluator.print_evaluation_report(results)
```

### Expected Metrics

| Metric | Score Range | Good Performance |
|--------|-------------|------------------|
| BLEU-1 | 0.0 - 1.0   | > 0.65          |
| BLEU-2 | 0.0 - 1.0   | > 0.45          |
| BLEU-3 | 0.0 - 1.0   | > 0.30          |
| BLEU-4 | 0.0 - 1.0   | > 0.20          |
| METEOR | 0.0 - 1.0   | > 0.25          |
| ROUGE-L| 0.0 - 1.0   | > 0.50          |

## ğŸ¨ Attention Visualization

```python
from attention_visualization import AttentionVisualizer

visualizer = AttentionVisualizer()

# Visualize attention on image
visualizer.visualize_attention_on_image(
    image_path='example.jpg',
    caption='a dog playing with a ball',
    attention_weights=attention_weights,
    save_path='attention_viz.png'
)

# Plot attention statistics
visualizer.plot_attention_statistics(
    attention_weights=attention_weights,
    caption='a dog playing with a ball'
)
```

## ğŸ”„ Data Augmentation

### Image Augmentation

```python
from data_augmentation import ImageAugmentation

img_aug = ImageAugmentation(
    img_size=224,
    color_jitter=True,
    random_crop=True,
    random_flip=True
)

augmented_image = img_aug(image, training=True)
```

### Caption Augmentation

```python
from data_augmentation import CaptionAugmentation

cap_aug = CaptionAugmentation(augment_prob=0.3)

# Generate augmented versions
augmented_captions = cap_aug(
    "a dog playing in the park",
    n_augmented=3
)
```

## ğŸ“ˆ Monitoring Training

The training script logs:
- Training loss per epoch
- Validation loss per epoch
- BLEU scores on validation set
- Learning rate schedule
- Best model checkpoints

Visualize training progress:
```python
python utils.py --plot_training --log_file training_log.txt
```

## ğŸ§ª Experiments

The `experiments/` directory contains Jupyter notebooks for:
- Model architecture comparisons
- Hyperparameter tuning
- Ablation studies
- Qualitative analysis

## ğŸ“ Notebooks

### 1. data_and_training.ipynb
- Data exploration and preprocessing
- Model definition and architecture
- Training loop implementation
- Model checkpointing

### 2. inference.ipynb
- Load trained model
- Generate captions for test images
- Qualitative analysis
- Error analysis and failure cases

## ğŸ† Results

### Quantitative Results

| Model | BLEU-4 | METEOR | Training Time |
|-------|--------|--------|---------------|
| Baseline | 0.185 | 0.223 | 4h |
| + Attention | 0.234 | 0.267 | 5h |
| + Augmentation | 0.251 | 0.281 | 6h |

### Qualitative Examples

**Example 1:**
- Image: Beach scene with surfer
- Generated: "a person surfing on a wave in the ocean"
- Reference: "a surfer riding a large wave at sunset"

**Example 2:**
- Image: Cat on windowsill
- Generated: "a cat sitting on a window looking outside"
- Reference: "an orange cat sitting by a window"

## ğŸ› Common Issues

### Out of Memory Error
```bash
# Reduce batch size
python train.py --batch_size 16

# Or use gradient accumulation
python train.py --batch_size 32 --accumulation_steps 2
```

### Poor Caption Quality
- Increase training epochs
- Try different learning rates
- Add more data augmentation
- Use pre-trained word embeddings (GloVe, Word2Vec)

### Slow Training
- Use GPU acceleration
- Enable mixed precision training
- Optimize data loading with multiple workers
- Use a lighter encoder (e.g., MobileNet instead of ResNet101)

## ğŸ¤ Contributing

Contributions are welcome! Each team member should:

1. Create a feature branch
```bash
git checkout -b feature/your-feature-name
```

2. Make commits with clear messages
```bash
git commit -m "Add attention visualization module"
```

3. Push changes
```bash
git push origin feature/your-feature-name
```

4. Create a Pull Request

### Contribution Guidelines

- Write clear, documented code
- Add unit tests for new features
- Update README with new functionality
- Follow PEP 8 style guide

## ğŸ“š References

- [Show, Attend and Tell](https://arxiv.org/abs/1502.03044)
- [Bottom-Up and Top-Down Attention](https://arxiv.org/abs/1707.07998)
- [Image Captioning with Semantic Attention](https://arxiv.org/abs/1502.03044)

## ğŸ‘¥ Team Members

- Member 1 - [Contribution focus]
- Member 2 - [Contribution focus]
- Member 3 - [Contribution focus]

## ğŸ“„ License

This project is for educational purposes as part of the Deep Learning course final project.

## ğŸ™ Acknowledgments

- Course instructors and TAs
- Flickr8k dataset creators
- PyTorch community

---

**Last Updated:** January 2026
**Course:** Deep Learning Final Project
**Due Date:** January 23, 2026
